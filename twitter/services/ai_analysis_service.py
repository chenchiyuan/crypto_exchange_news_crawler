import json
import logging
from decimal import Decimal
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from pathlib import Path

from django.conf import settings
from twitter.sdk.deepseek_sdk import DeepSeekSDK, DeepSeekAPIError, DeepSeekResponse
from twitter.models import Tweet


logger = logging.getLogger(__name__)


class AIAnalysisService:
    """
    AI åˆ†ææœåŠ¡

    è´Ÿè´£è°ƒç”¨ DeepSeek AI API åˆ†ææ¨æ–‡å†…å®¹ï¼Œæ”¯æŒæ‰¹æ¬¡å’Œä¸€æ¬¡æ€§åˆ†ææ¨¡å¼ã€‚
    """

    # é»˜è®¤æ‰¹æ¬¡å¤§å°
    DEFAULT_BATCH_SIZE = 100

    # é»˜è®¤ prompt æ¨¡æ¿è·¯å¾„
    DEFAULT_PROMPT_TEMPLATE = 'twitter/templates/prompts/crypto_analysis.txt'

    def __init__(self, deepseek_sdk: DeepSeekSDK = None):
        """
        åˆå§‹åŒ– AI åˆ†ææœåŠ¡

        Args:
            deepseek_sdk: DeepSeekSDK å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨åˆ›å»º
        """
        self.sdk = deepseek_sdk or DeepSeekSDK()

    def load_prompt_template(self, template_path: str = None) -> str:
        """
        åŠ è½½ prompt æ¨¡æ¿

        Args:
            template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿

        Returns:
            str: æ¨¡æ¿å†…å®¹

        Raises:
            FileNotFoundError: æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨
        """
        if template_path is None:
            template_path = self.DEFAULT_PROMPT_TEMPLATE

        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not Path(template_path).is_absolute():
            template_path = Path(settings.BASE_DIR) / template_path

        if not Path(template_path).exists():
            raise FileNotFoundError(f"Prompt template not found: {template_path}")

        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()

    def format_tweets_for_analysis(self, tweets: List[Tweet]) -> str:
        """
        æ ¼å¼åŒ–æ¨æ–‡åˆ—è¡¨ä¸ºåˆ†ææ–‡æœ¬

        Args:
            tweets: æ¨æ–‡å¯¹è±¡åˆ—è¡¨

        Returns:
            str: æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        formatted_lines = []

        for i, tweet in enumerate(tweets, 1):
            formatted_lines.append(
                f"{i}. [@{tweet.screen_name}] ({tweet.tweet_created_at.strftime('%Y-%m-%d %H:%M')})\n"
                f"   å†…å®¹: {tweet.content}\n"
                f"   äº’åŠ¨: ğŸ‘{tweet.favorite_count} ğŸ”„{tweet.retweet_count} ğŸ’¬{tweet.reply_count}\n"
                f"   Tweet ID: {tweet.tweet_id}\n"
            )

        return "\n".join(formatted_lines)

    def estimate_analysis_cost(self, tweets: List[Tweet],
                              prompt_template: str = None) -> Tuple[int, Decimal]:
        """
        ä¼°ç®—åˆ†ææˆæœ¬

        Args:
            tweets: è¦åˆ†æçš„æ¨æ–‡åˆ—è¡¨
            prompt_template: Prompt æ¨¡æ¿å†…å®¹ï¼ˆå¯é€‰ï¼‰

        Returns:
            Tuple[int, Decimal]: (é¢„ä¼° token æ•°, é¢„ä¼°æˆæœ¬)
        """
        # åŠ è½½ prompt æ¨¡æ¿
        if prompt_template is None:
            prompt_template = self.load_prompt_template()

        # æ ¼å¼åŒ–æ¨æ–‡å†…å®¹
        tweets_text = self.format_tweets_for_analysis(tweets)

        # è®¡ç®—æ€»æ–‡æœ¬é•¿åº¦
        full_text = prompt_template + "\n\n" + tweets_text

        # ä¼°ç®— token æ•°
        estimated_tokens = self.sdk.count_tokens(full_text)

        # ä¼°ç®—æˆæœ¬ï¼ˆåŒ…å«è¾“å…¥å’Œé¢„ä¼°è¾“å‡ºï¼‰
        estimated_cost = self.sdk.estimate_cost(estimated_tokens)

        logger.info(f"æˆæœ¬ä¼°ç®—: {len(tweets)} æ¡æ¨æ–‡, "
                   f"çº¦ {estimated_tokens} tokens, "
                   f"é¢„ä¼°æˆæœ¬ ${estimated_cost:.4f}")

        return estimated_tokens, estimated_cost

    def analyze_tweets_once(self, tweets: List[Tweet],
                           prompt_template: str,
                           task_id: str = None,
                           save_prompt: bool = False) -> Dict:
        """
        ä¸€æ¬¡æ€§åˆ†ææ¨æ–‡ï¼ˆé€‚ç”¨äºå°‘é‡æ¨æ–‡ <100 æ¡ï¼‰

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            prompt_template: Prompt æ¨¡æ¿å†…å®¹
            task_id: ä»»åŠ¡ IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            save_prompt: æ˜¯å¦ä¿å­˜æ¨é€ç»™AIå‰çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Returns:
            Dict: åˆ†æç»“æœå­—å…¸

        Raises:
            DeepSeekAPIError: API è°ƒç”¨å¤±è´¥
            ValueError: è§£æç»“æœå¤±è´¥
        """
        logger.info(f"[Task {task_id}] å¼€å§‹ä¸€æ¬¡æ€§åˆ†æ {len(tweets)} æ¡æ¨æ–‡")

        # æ ¼å¼åŒ–æ¨æ–‡å†…å®¹
        tweets_text = self.format_tweets_for_analysis(tweets)

        # è°ƒç”¨ AI API
        response = self.sdk.analyze_content(
            content=tweets_text,
            prompt_template=prompt_template,
            task_id=task_id
        )

        # ä¿å­˜åŸå§‹promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if save_prompt:
            self._save_prompt_for_debug(
                task_id=task_id,
                prompt_template=prompt_template,
                tweets_text=tweets_text,
                final_prompt=response.content if hasattr(response, 'content') else str(response)
            )

        # è§£æ JSON ç»“æœ
        analysis_result = self._parse_ai_response(response.content)

        # æ·»åŠ å…ƒæ•°æ®
        analysis_result['analysis_metadata'] = {
            'total_tweets': len(tweets),
            'analysis_timestamp': datetime.now().isoformat(),
            'time_range': f"{tweets[0].tweet_created_at.isoformat()} ~ {tweets[-1].tweet_created_at.isoformat()}" if tweets else "N/A",
            'tokens_used': response.tokens_used,
            'actual_cost': float(response.cost_estimate),
            'processing_time_ms': response.processing_time_ms,
            'model': response.model
        }

        logger.info(f"[Task {task_id}] åˆ†æå®Œæˆ, æˆæœ¬: ${response.cost_estimate:.4f}, "
                   f"è€—æ—¶: {response.processing_time_ms}ms")

        return analysis_result

    def analyze_tweets_batch(self, tweets: List[Tweet],
                            prompt_template: str,
                            batch_size: int = DEFAULT_BATCH_SIZE,
                            task_id: str = None,
                            save_prompt: bool = False) -> Dict:
        """
        åˆ†æ‰¹åˆ†ææ¨æ–‡ï¼ˆé€‚ç”¨äºå¤§é‡æ¨æ–‡ â‰¥100 æ¡ï¼‰

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            prompt_template: Prompt æ¨¡æ¿å†…å®¹
            batch_size: æ¯æ‰¹æ¨æ–‡æ•°é‡
            task_id: ä»»åŠ¡ IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            save_prompt: æ˜¯å¦ä¿å­˜æ¨é€ç»™AIå‰çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Returns:
            Dict: åˆå¹¶åçš„åˆ†æç»“æœå­—å…¸

        Raises:
            DeepSeekAPIError: API è°ƒç”¨å¤±è´¥
            ValueError: è§£æç»“æœå¤±è´¥
        """
        logger.info(f"[Task {task_id}] å¼€å§‹åˆ†æ‰¹åˆ†æ {len(tweets)} æ¡æ¨æ–‡, "
                   f"æ‰¹æ¬¡å¤§å°: {batch_size}")

        batch_results = []
        total_cost = Decimal('0.0000')
        total_tokens = 0
        total_time_ms = 0

        # ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€æ‰¹ï¼‰
        if save_prompt:
            first_batch_tweets = tweets[:batch_size]
            tweets_text = self.format_tweets_for_analysis(first_batch_tweets)
            self._save_prompt_for_debug(
                task_id=f"{task_id}_batch_1",
                prompt_template=prompt_template,
                tweets_text=tweets_text,
                final_prompt="[æ‰¹æ¬¡æ¨¡å¼: å¤šä¸ªæ‰¹æ¬¡åˆå¹¶ç»“æœ]"
            )

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(tweets), batch_size):
            batch = tweets[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(tweets) + batch_size - 1) // batch_size

            logger.info(f"[Task {task_id}] å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} "
                       f"({len(batch)} æ¡æ¨æ–‡)")

            # åˆ†æå•ä¸ªæ‰¹æ¬¡
            batch_result = self.analyze_tweets_once(
                tweets=batch,
                prompt_template=prompt_template,
                task_id=f"{task_id}_batch{batch_num}"
            )

            batch_results.append(batch_result)

            # ç´¯è®¡ç»Ÿè®¡
            metadata = batch_result.get('analysis_metadata', {})
            total_cost += Decimal(str(metadata.get('actual_cost', 0)))
            total_tokens += metadata.get('tokens_used', 0)
            total_time_ms += metadata.get('processing_time_ms', 0)

        # åˆå¹¶æ‰¹æ¬¡ç»“æœ
        merged_result = self._merge_batch_results(batch_results)

        # æ›´æ–°åˆå¹¶åçš„å…ƒæ•°æ®
        merged_result['analysis_metadata'] = {
            'total_tweets': len(tweets),
            'analysis_timestamp': datetime.now().isoformat(),
            'time_range': f"{tweets[0].tweet_created_at.isoformat()} ~ {tweets[-1].tweet_created_at.isoformat()}" if tweets else "N/A",
            'tokens_used': total_tokens,
            'actual_cost': float(total_cost),
            'processing_time_ms': total_time_ms,
            'batch_count': len(batch_results),
            'batch_size': batch_size,
            'model': self.sdk.model
        }

        logger.info(f"[Task {task_id}] æ‰¹æ¬¡åˆ†æå®Œæˆ, æ€»æˆæœ¬: ${total_cost:.4f}, "
                   f"æ€»è€—æ—¶: {total_time_ms}ms, æ‰¹æ¬¡æ•°: {len(batch_results)}")

        return merged_result

    def analyze_tweets(self, tweets: List[Tweet],
                      prompt_template: str = None,
                      batch_mode: bool = None,
                      batch_size: int = DEFAULT_BATCH_SIZE,
                      task_id: str = None,
                      save_prompt: bool = False) -> Dict:
        """
        åˆ†ææ¨æ–‡ï¼ˆè‡ªåŠ¨é€‰æ‹©æ‰¹æ¬¡æˆ–ä¸€æ¬¡æ€§æ¨¡å¼ï¼‰

        Args:
            tweets: æ¨æ–‡åˆ—è¡¨
            prompt_template: Prompt æ¨¡æ¿å†…å®¹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é¢„è®¾æ¨¡æ¿ï¼‰
            batch_mode: æ˜¯å¦ä½¿ç”¨æ‰¹æ¬¡æ¨¡å¼ï¼ˆNone=è‡ªåŠ¨åˆ¤æ–­ï¼ŒTrue=å¼ºåˆ¶æ‰¹æ¬¡ï¼ŒFalse=å¼ºåˆ¶ä¸€æ¬¡æ€§ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä»…æ‰¹æ¬¡æ¨¡å¼æœ‰æ•ˆï¼‰
            task_id: ä»»åŠ¡ ID
            save_prompt: æ˜¯å¦ä¿å­˜æ¨é€ç»™AIå‰çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Returns:
            Dict: åˆ†æç»“æœå­—å…¸
        """
        if not tweets:
            raise ValueError("æ¨æ–‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        # åŠ è½½ prompt æ¨¡æ¿
        if prompt_template is None:
            prompt_template = self.load_prompt_template()

        # è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼
        if batch_mode is None:
            batch_mode = len(tweets) >= self.DEFAULT_BATCH_SIZE

        # æ‰§è¡Œåˆ†æ
        if batch_mode:
            return self.analyze_tweets_batch(
                tweets=tweets,
                prompt_template=prompt_template,
                batch_size=batch_size,
                task_id=task_id,
                save_prompt=save_prompt
            )
        else:
            return self.analyze_tweets_once(
                tweets=tweets,
                prompt_template=prompt_template,
                task_id=task_id,
                save_prompt=save_prompt
            )

    def _parse_ai_response(self, response_text: str) -> Dict:
        """
        è§£æ AI è¿”å›çš„ JSON ç»“æœ

        Args:
            response_text: AI è¿”å›çš„æ–‡æœ¬

        Returns:
            Dict: è§£æåçš„ç»“æœå­—å…¸

        Raises:
            ValueError: JSON è§£æå¤±è´¥
        """
        # å°è¯•æå– JSON éƒ¨åˆ†ï¼ˆAI å¯èƒ½è¿”å›é¢å¤–çš„æ–‡æœ¬ï¼‰
        try:
            # ç›´æ¥è§£æ
            return json.loads(response_text)
        except json.JSONDecodeError:
            # å°è¯•æå– {} ä¹‹é—´çš„å†…å®¹
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass

            logger.error(f"æ— æ³•è§£æ AI å“åº”ä¸º JSON: {response_text[:200]}")
            raise ValueError(f"AI è¿”å›çš„ç»“æœä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼")

    def _merge_batch_results(self, batch_results: List[Dict]) -> Dict:
        """
        åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„åˆ†æç»“æœ

        Args:
            batch_results: æ‰¹æ¬¡ç»“æœåˆ—è¡¨

        Returns:
            Dict: åˆå¹¶åçš„ç»“æœ
        """
        if not batch_results:
            return {}

        if len(batch_results) == 1:
            return batch_results[0]

        # åˆå¹¶æƒ…ç»ªç»Ÿè®¡
        total_bullish = sum(r.get('sentiment', {}).get('bullish', 0) for r in batch_results)
        total_bearish = sum(r.get('sentiment', {}).get('bearish', 0) for r in batch_results)
        total_neutral = sum(r.get('sentiment', {}).get('neutral', 0) for r in batch_results)
        total_tweets = total_bullish + total_bearish + total_neutral

        merged_sentiment = {
            'bullish': total_bullish,
            'bearish': total_bearish,
            'neutral': total_neutral,
            'bullish_percentage': round(total_bullish / total_tweets * 100, 2) if total_tweets > 0 else 0,
            'bearish_percentage': round(total_bearish / total_tweets * 100, 2) if total_tweets > 0 else 0,
            'neutral_percentage': round(total_neutral / total_tweets * 100, 2) if total_tweets > 0 else 0,
        }

        # åˆå¹¶å…³é”®è¯é¢˜ï¼ˆå»é‡å¹¶é‡æ–°ç»Ÿè®¡ï¼‰
        topic_counts = {}
        for result in batch_results:
            for topic in result.get('key_topics', []):
                topic_name = topic['topic']
                if topic_name in topic_counts:
                    topic_counts[topic_name]['count'] += topic['count']
                else:
                    topic_counts[topic_name] = {
                        'topic': topic_name,
                        'count': topic['count'],
                        'sentiment': topic.get('sentiment', 'neutral')
                    }

        merged_topics = sorted(topic_counts.values(), key=lambda x: x['count'], reverse=True)[:10]

        # åˆå¹¶é‡è¦æ¨æ–‡ï¼ˆæŒ‰äº’åŠ¨é‡æ’åºï¼Œå–å‰ 10ï¼‰
        all_important_tweets = []
        for result in batch_results:
            all_important_tweets.extend(result.get('important_tweets', []))

        merged_important_tweets = sorted(
            all_important_tweets,
            key=lambda x: x.get('engagement', 0),
            reverse=True
        )[:10]

        # åˆå¹¶å¸‚åœºæ€»ç»“ï¼ˆæ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„æ€»ç»“ï¼‰
        market_summaries = [r.get('market_summary', '') for r in batch_results if r.get('market_summary')]
        merged_summary = ' '.join(market_summaries) if market_summaries else "å¤šæ‰¹æ¬¡åˆ†æå®Œæˆ"

        return {
            'sentiment': merged_sentiment,
            'key_topics': merged_topics,
            'important_tweets': merged_important_tweets,
            'market_summary': merged_summary,
            'analysis_metadata': {}  # ä¼šåœ¨è°ƒç”¨æ–¹æ›´æ–°
        }

    def _save_prompt_for_debug(self, task_id: str, prompt_template: str,
                               tweets_text: str, final_prompt: str):
        """
        ä¿å­˜æ¨é€ç»™AIå‰çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            task_id: ä»»åŠ¡ID
            prompt_template: æç¤ºè¯æ¨¡æ¿
            tweets_text: æ ¼å¼åŒ–åçš„æ¨æ–‡å†…å®¹
            final_prompt: æœ€ç»ˆå‘é€ç»™AIçš„å®Œæ•´prompt
        """
        try:
            import os
            from pathlib import Path

            # åˆ›å»ºdebugç›®å½•
            debug_dir = Path(settings.BASE_DIR) / 'debug_prompts'
            debug_dir.mkdir(exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"prompt_{task_id}_{timestamp}.txt"
            filepath = debug_dir / filename

            # æ„å»ºä¿å­˜å†…å®¹
            content = []
            content.append("=" * 80)
            content.append(f"AI è°ƒè¯•ä¿¡æ¯ - Task: {task_id}")
            content.append(f"ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            content.append("=" * 80)
            content.append("")

            content.append("ã€1. æç¤ºè¯æ¨¡æ¿ã€‘")
            content.append("-" * 80)
            content.append(prompt_template)
            content.append("")

            content.append("ã€2. æ¨æ–‡åŸæ–‡å†…å®¹ã€‘")
            content.append("-" * 80)
            content.append(tweets_text)
            content.append("")

            content.append("ã€3. æœ€ç»ˆå‘é€ç»™AIçš„å®Œæ•´Promptã€‘")
            content.append("-" * 80)
            content.append(final_prompt)
            content.append("")

            content.append("=" * 80)
            content.append("è°ƒè¯•ä¿¡æ¯ç»“æŸ")
            content.append("=" * 80)

            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))

            logger.info(f"[Task {task_id}] âœ… è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ°: {filepath}")

        except Exception as e:
            logger.error(f"[Task {task_id}] âŒ ä¿å­˜è°ƒè¯•ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
