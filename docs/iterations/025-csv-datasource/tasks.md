# ä»»åŠ¡è®¡åˆ’: CSVæ•°æ®æºKçº¿è·å–æœåŠ¡

**è¿­ä»£ç¼–å·**: 025
**ç‰ˆæœ¬**: 1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-01-09
**çŠ¶æ€**: ä»»åŠ¡è§„åˆ’å®Œæˆ

---

## ä»»åŠ¡æ¦‚è§ˆ

| é˜¶æ®µ | ä»»åŠ¡æ•° | åŠŸèƒ½ç‚¹è¦†ç›– | çŠ¶æ€ |
|------|--------|------------|------|
| é˜¶æ®µ1: æ ¸å¿ƒè§£æå±‚ | 4 | FP-001~004 | å¾…å¼€å§‹ |
| é˜¶æ®µ2: æ•°æ®è·å–å±‚ | 4 | FP-005~010 | å¾…å¼€å§‹ |
| é˜¶æ®µ3: å·¥å‚é›†æˆå±‚ | 3 | FP-011~014 | å¾…å¼€å§‹ |
| é˜¶æ®µ4: ç«¯åˆ°ç«¯éªŒè¯ | 3 | FP-015~022 | å¾…å¼€å§‹ |
| **æ€»è®¡** | **14** | **22ä¸ªP0** | |

---

## é˜¶æ®µ1: æ ¸å¿ƒè§£æå±‚

### TASK-025-001: åˆ›å»ºCSVParseråŸºç¡€ç±»

**ç›®æ ‡**: å®ç°CSVæ–‡ä»¶è§£æå™¨ï¼Œè§£æå¸å®‰12åˆ—æ ¼å¼

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-001, FP-002, FP-003, FP-004

**æ–‡ä»¶å˜æ›´**:
- ğŸ†• `ddps_z/datasources/csv_parser.py`

**å®ç°è¦ç‚¹**:
```python
class CSVParser:
    """CSVæ–‡ä»¶è§£æå™¨"""

    COLUMN_MAPPING = {
        'timestamp': 0,
        'open': 1,
        'high': 2,
        'low': 3,
        'close': 4,
        'volume': 5
    }

    def __init__(self, timestamp_unit: str = 'microseconds'):
        self.timestamp_unit = timestamp_unit
        self._divisor = self._get_divisor()

    def _get_divisor(self) -> int:
        """è·å–æ—¶é—´æˆ³è½¬æ¢é™¤æ•°"""
        divisors = {
            'microseconds': 1000,
            'milliseconds': 1,
            'seconds': 0.001
        }
        return divisors.get(self.timestamp_unit, 1000)

    def parse(self, csv_path: str) -> List[StandardKLine]:
        """è§£æCSVæ–‡ä»¶"""
        # 1. ä½¿ç”¨pandasè¯»å–ï¼ˆæ— è¡¨å¤´ï¼‰
        # 2. æå–æ‰€éœ€åˆ—
        # 3. æ—¶é—´æˆ³è½¬æ¢
        # 4. è½¬æ¢ä¸ºStandardKLineåˆ—è¡¨
        pass
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æˆåŠŸè¯»å–æ— è¡¨å¤´CSVæ–‡ä»¶
- [ ] æ­£ç¡®è§£æ12åˆ—å¸å®‰æ ¼å¼
- [ ] æ—¶é—´æˆ³æ­£ç¡®ä»å¾®ç§’è½¬æ¢ä¸ºæ¯«ç§’
- [ ] è¿”å›List[StandardKLine]æ ¼å¼

**ä¾èµ–**: æ— 

---

### TASK-025-002: å®ç°CSVParser.parse()æ–¹æ³•

**ç›®æ ‡**: å®ŒæˆCSVè§£ææ ¸å¿ƒé€»è¾‘

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-001, FP-002, FP-003

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/csv_parser.py`

**å®ç°è¦ç‚¹**:
```python
def parse(self, csv_path: str) -> List[StandardKLine]:
    """è§£æCSVæ–‡ä»¶ï¼Œè¿”å›StandardKLineåˆ—è¡¨"""
    import pandas as pd
    from pathlib import Path

    # 1. éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not Path(csv_path).exists():
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

    # 2. è¯»å–CSVï¼ˆæ— è¡¨å¤´ï¼‰
    df = pd.read_csv(csv_path, header=None)

    # 3. æå–æ‰€éœ€åˆ—å¹¶è½¬æ¢
    klines = []
    for _, row in df.iterrows():
        timestamp = int(row[0] // self._divisor)  # å¾®ç§’â†’æ¯«ç§’
        kline = StandardKLine(
            timestamp=timestamp,
            open=float(row[1]),
            high=float(row[2]),
            low=float(row[3]),
            close=float(row[4]),
            volume=float(row[5])
        )
        klines.append(kline)

    return klines
```

**ä¼˜åŒ–æ–¹å‘**:
- ä½¿ç”¨numpyå‘é‡åŒ–æ“ä½œæ›¿ä»£é€è¡Œè¿­ä»£
- é¢„åˆ†é…åˆ—è¡¨å¤§å°

**éªŒæ”¶æ ‡å‡†**:
- [ ] 268ä¸‡è¡ŒCSVåŠ è½½æ—¶é—´<30ç§’
- [ ] å†…å­˜å ç”¨<2GB
- [ ] æ•°æ®è½¬æ¢æ­£ç¡®æ€§100%

**ä¾èµ–**: TASK-025-001

---

### TASK-025-003: ç¼–å†™CSVParserå•å…ƒæµ‹è¯•

**ç›®æ ‡**: éªŒè¯CSVParserå„åŠŸèƒ½ç‚¹

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-001, FP-002, FP-003, FP-004

**æ–‡ä»¶å˜æ›´**:
- ğŸ†• `ddps_z/tests/test_csv_parser.py`

**æµ‹è¯•ç”¨ä¾‹**:
```python
class TestCSVParser:
    def test_parse_binance_format(self):
        """æµ‹è¯•è§£æå¸å®‰12åˆ—æ ¼å¼"""
        pass

    def test_timestamp_conversion_microseconds(self):
        """æµ‹è¯•å¾®ç§’æ—¶é—´æˆ³è½¬æ¢"""
        pass

    def test_timestamp_conversion_milliseconds(self):
        """æµ‹è¯•æ¯«ç§’æ—¶é—´æˆ³è½¬æ¢"""
        pass

    def test_no_header_csv(self):
        """æµ‹è¯•æ— è¡¨å¤´CSV"""
        pass

    def test_file_not_found(self):
        """æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨å¼‚å¸¸"""
        pass
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%

**ä¾èµ–**: TASK-025-002

---

### TASK-025-004: æ€§èƒ½ä¼˜åŒ–CSVParser

**ç›®æ ‡**: ä½¿ç”¨numpyå‘é‡åŒ–ä¼˜åŒ–è§£ææ€§èƒ½

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-001, FP-002

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/csv_parser.py`

**ä¼˜åŒ–å®ç°**:
```python
def parse(self, csv_path: str) -> List[StandardKLine]:
    """ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨numpyå‘é‡åŒ–"""
    import pandas as pd
    import numpy as np

    # è¯»å–CSV
    df = pd.read_csv(csv_path, header=None, usecols=[0, 1, 2, 3, 4, 5])

    # å‘é‡åŒ–è½¬æ¢æ—¶é—´æˆ³
    timestamps = (df[0].values // self._divisor).astype(np.int64)

    # æ‰¹é‡åˆ›å»ºStandardKLine
    klines = [
        StandardKLine(
            timestamp=int(timestamps[i]),
            open=float(df.iloc[i, 1]),
            high=float(df.iloc[i, 2]),
            low=float(df.iloc[i, 3]),
            close=float(df.iloc[i, 4]),
            volume=float(df.iloc[i, 5])
        )
        for i in range(len(df))
    ]

    return klines
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] 268ä¸‡è¡ŒåŠ è½½æ—¶é—´<15ç§’
- [ ] å†…å­˜å³°å€¼<500MB

**ä¾èµ–**: TASK-025-002

---

## é˜¶æ®µ2: æ•°æ®è·å–å±‚

### TASK-025-005: åˆ›å»ºCSVFetcheråŸºç¡€ç±»

**ç›®æ ‡**: å®ç°CSVFetcherï¼Œç»§æ‰¿KLineFetcher

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-005, FP-009, FP-010

**æ–‡ä»¶å˜æ›´**:
- ğŸ†• `ddps_z/datasources/csv_fetcher.py`

**å®ç°è¦ç‚¹**:
```python
from ddps_z.datasources.base import KLineFetcher
from ddps_z.datasources.csv_parser import CSVParser
from ddps_z.models import StandardKLine

class CSVFetcher(KLineFetcher):
    """CSVæ–‡ä»¶Kçº¿æ•°æ®è·å–å™¨"""

    SUPPORTED_INTERVALS = ['1s', '1m']

    def __init__(
        self,
        csv_path: str,
        interval: str = '1s',
        market_type: str = 'csv_local',
        timestamp_unit: str = 'microseconds'
    ):
        self._csv_path = csv_path
        self._interval = interval
        self._market_type = market_type
        self._cache = None
        self._parser = CSVParser(timestamp_unit=timestamp_unit)

    @property
    def market_type(self) -> str:
        return self._market_type

    def supports_interval(self, interval: str) -> bool:
        return interval in self.SUPPORTED_INTERVALS
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ­£ç¡®ç»§æ‰¿KLineFetcher
- [ ] market_typeè¿”å›'csv_local'
- [ ] supports_intervalæ­£ç¡®åˆ¤æ–­1s/1m

**ä¾èµ–**: TASK-025-001

---

### TASK-025-006: å®ç°CSVFetcher.fetch()æ–¹æ³•

**ç›®æ ‡**: å®ç°fetchæ–¹æ³•ï¼Œæ”¯æŒå…¨é‡åŠ è½½å’Œæ—¶é—´è¿‡æ»¤

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-006, FP-007, FP-008

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/csv_fetcher.py`

**å®ç°è¦ç‚¹**:
```python
def fetch(
    self,
    symbol: str,
    interval: str,
    limit: int = 500,
    start_time: Optional[int] = None,
    end_time: Optional[int] = None
) -> List[StandardKLine]:
    """ä»CSVåŠ è½½Kçº¿æ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)

    # 1. æ‡’åŠ è½½ï¼šé¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½CSV
    if self._cache is None:
        logger.info(f"é¦–æ¬¡åŠ è½½CSV: {self._csv_path}")
        self._cache = self._parser.parse(self._csv_path)
        logger.info(f"åŠ è½½å®Œæˆï¼Œå…± {len(self._cache)} æ ¹Kçº¿")

    # 2. æ—¶é—´èŒƒå›´è¿‡æ»¤
    result = self._filter_by_time(start_time, end_time)

    # 3. åº”ç”¨limit
    if limit and len(result) > limit:
        result = result[-limit:]

    return result

def _filter_by_time(
    self,
    start_time: Optional[int],
    end_time: Optional[int]
) -> List[StandardKLine]:
    """æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤"""
    if start_time is None and end_time is None:
        return self._cache.copy()

    result = []
    for kline in self._cache:
        if start_time and kline.timestamp < start_time:
            continue
        if end_time and kline.timestamp > end_time:
            continue
        result.append(kline)

    return result
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] é¦–æ¬¡è°ƒç”¨è§¦å‘CSVåŠ è½½
- [ ] åç»­è°ƒç”¨ä½¿ç”¨ç¼“å­˜
- [ ] æ—¶é—´èŒƒå›´è¿‡æ»¤æ­£ç¡®
- [ ] limitå‚æ•°ç”Ÿæ•ˆ

**ä¾èµ–**: TASK-025-005

---

### TASK-025-007: ç¼–å†™CSVFetcherå•å…ƒæµ‹è¯•

**ç›®æ ‡**: éªŒè¯CSVFetcherå„åŠŸèƒ½ç‚¹

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-005~010

**æ–‡ä»¶å˜æ›´**:
- ğŸ†• `ddps_z/tests/test_csv_fetcher.py`

**æµ‹è¯•ç”¨ä¾‹**:
```python
class TestCSVFetcher:
    def test_inherits_kline_fetcher(self):
        """æµ‹è¯•æ­£ç¡®ç»§æ‰¿KLineFetcher"""
        pass

    def test_market_type_property(self):
        """æµ‹è¯•market_typeè¿”å›csv_local"""
        pass

    def test_supports_interval_1s(self):
        """æµ‹è¯•æ”¯æŒ1så‘¨æœŸ"""
        pass

    def test_supports_interval_1m(self):
        """æµ‹è¯•æ”¯æŒ1må‘¨æœŸ"""
        pass

    def test_fetch_lazy_load(self):
        """æµ‹è¯•æ‡’åŠ è½½æœºåˆ¶"""
        pass

    def test_fetch_cache_hit(self):
        """æµ‹è¯•ç¼“å­˜å‘½ä¸­"""
        pass

    def test_fetch_time_filter(self):
        """æµ‹è¯•æ—¶é—´èŒƒå›´è¿‡æ»¤"""
        pass

    def test_fetch_limit(self):
        """æµ‹è¯•limitå‚æ•°"""
        pass
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%

**ä¾èµ–**: TASK-025-006

---

### TASK-025-008: ä¼˜åŒ–æ—¶é—´è¿‡æ»¤æ€§èƒ½

**ç›®æ ‡**: ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–æ—¶é—´èŒƒå›´è¿‡æ»¤

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-008

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/csv_fetcher.py`

**ä¼˜åŒ–å®ç°**:
```python
import bisect

def _filter_by_time(
    self,
    start_time: Optional[int],
    end_time: Optional[int]
) -> List[StandardKLine]:
    """ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–æ—¶é—´è¿‡æ»¤"""
    if start_time is None and end_time is None:
        return self._cache.copy()

    # æå–æ—¶é—´æˆ³åˆ—è¡¨ç”¨äºäºŒåˆ†æŸ¥æ‰¾
    timestamps = [k.timestamp for k in self._cache]

    # äºŒåˆ†æŸ¥æ‰¾èµ·å§‹ä½ç½®
    start_idx = 0
    if start_time:
        start_idx = bisect.bisect_left(timestamps, start_time)

    # äºŒåˆ†æŸ¥æ‰¾ç»“æŸä½ç½®
    end_idx = len(self._cache)
    if end_time:
        end_idx = bisect.bisect_right(timestamps, end_time)

    return self._cache[start_idx:end_idx]
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ—¶é—´è¿‡æ»¤ä»O(n)ä¼˜åŒ–åˆ°O(log n)
- [ ] è¿‡æ»¤100ä¸‡æ¡æ•°æ®<10ms

**ä¾èµ–**: TASK-025-006

---

## é˜¶æ®µ3: å·¥å‚é›†æˆå±‚

### TASK-025-009: æ‰©å±•FetcherFactoryæ”¯æŒkwargs

**ç›®æ ‡**: ä¿®æ”¹createæ–¹æ³•æ”¯æŒä¼ é€’kwargsç»™Fetcheræ„é€ å‡½æ•°

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-011, FP-012

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/fetcher_factory.py`

**å®ç°è¦ç‚¹**:
```python
# åœ¨_registryä¸­æ³¨å†Œcsv_local
_registry: Dict[str, Type[KLineFetcher]] = {
    MarketType.CRYPTO_SPOT.value: BinanceFetcher,
    MarketType.CRYPTO_FUTURES.value: BinanceFetcher,
    'csv_local': CSVFetcher,  # æ–°å¢
}

@classmethod
def create(cls, market_type: str, **kwargs) -> KLineFetcher:
    """
    åˆ›å»ºFetcherå®ä¾‹ï¼ˆæ‰©å±•ç‰ˆï¼‰

    Args:
        market_type: å¸‚åœºç±»å‹
        **kwargs: ä¼ é€’ç»™Fetcheræ„é€ å‡½æ•°çš„å‚æ•°
    """
    # å¯¹äºæ—§æ ¼å¼market_typeï¼Œè¿›è¡Œæ ‡å‡†åŒ–
    if market_type != 'csv_local':
        normalized = MarketType.normalize(market_type)
    else:
        normalized = market_type

    fetcher_class = cls._registry.get(normalized)

    if fetcher_class is None:
        raise ValueError(f"ä¸æ”¯æŒçš„å¸‚åœºç±»å‹: {market_type}")

    # æ ¹æ®ç±»å‹å†³å®šæ˜¯å¦ä¼ é€’kwargs
    if normalized == 'csv_local':
        return fetcher_class(**kwargs)
    else:
        # æ—§Fetcherä¿æŒå‘åå…¼å®¹
        return fetcher_class(market_type=normalized)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] csv_localæ³¨å†ŒæˆåŠŸ
- [ ] create('csv_local', csv_path=xxx)æ­£å¸¸å·¥ä½œ
- [ ] ç°æœ‰BinanceFetcherè°ƒç”¨ä¸å—å½±å“ï¼ˆå‘åå…¼å®¹ï¼‰

**ä¾èµ–**: TASK-025-005

---

### TASK-025-010: æ›´æ–°datasourcesæ¨¡å—å¯¼å‡º

**ç›®æ ‡**: åœ¨__init__.pyä¸­å¯¼å‡ºCSVFetcher

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-011

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `ddps_z/datasources/__init__.py`

**å®ç°è¦ç‚¹**:
```python
from ddps_z.datasources.base import KLineFetcher, FetchError
from ddps_z.datasources.binance_fetcher import BinanceFetcher
from ddps_z.datasources.csv_fetcher import CSVFetcher  # æ–°å¢
from ddps_z.datasources.fetcher_factory import FetcherFactory
from ddps_z.datasources.repository import KLineRepository

__all__ = [
    'KLineFetcher',
    'FetchError',
    'BinanceFetcher',
    'CSVFetcher',  # æ–°å¢
    'FetcherFactory',
    'KLineRepository',
]
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] `from ddps_z.datasources import CSVFetcher`å¯ç”¨

**ä¾èµ–**: TASK-025-005

---

### TASK-025-011: æ‰©å±•ProjectLoaderæ”¯æŒdata_sourceé…ç½®

**ç›®æ ‡**: è§£æJSONé…ç½®ä¸­çš„data_sourceèŠ‚ç‚¹

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-013

**æ–‡ä»¶å˜æ›´**:
- ğŸ“ `strategy_adapter/core/project_loader.py`

**å®ç°è¦ç‚¹**:
```python
def _parse_config(self, data: Dict[str, Any]) -> ProjectConfig:
    """è§£æé…ç½®æ•°æ®ï¼ˆæ‰©å±•ç‰ˆï¼‰"""

    # è§£ædata_sourceé…ç½®
    data_source_config = None
    if 'data_source' in data:
        ds = data['data_source']
        data_source_config = DataSourceConfig(
            type=ds.get('type', 'crypto_futures'),
            csv_path=ds.get('csv_path'),
            interval=ds.get('interval', '4h'),
            timestamp_unit=ds.get('timestamp_unit', 'microseconds')
        )

    # ... å…¶ä»–é…ç½®è§£æ

    return ProjectConfig(
        # ... å…¶ä»–å­—æ®µ
        data_source=data_source_config
    )
```

**æ–°å¢æ•°æ®ç±»**:
```python
@dataclass
class DataSourceConfig:
    """æ•°æ®æºé…ç½®"""
    type: str = 'crypto_futures'
    csv_path: Optional[str] = None
    interval: str = '4h'
    timestamp_unit: str = 'microseconds'
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ­£ç¡®è§£ædata_sourceé…ç½®
- [ ] ç¼ºå°‘data_sourceæ—¶ä½¿ç”¨é»˜è®¤å€¼
- [ ] csv_pathæ­£ç¡®ä¼ é€’

**ä¾èµ–**: TASK-025-009

---

## é˜¶æ®µ4: ç«¯åˆ°ç«¯éªŒè¯

### TASK-025-012: åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶

**ç›®æ ‡**: åˆ›å»ºCSVæ•°æ®æºå›æµ‹ç¤ºä¾‹é…ç½®

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-013

**æ–‡ä»¶å˜æ›´**:
- ğŸ†• `strategy_adapter/configs/csv_backtest_example.json`

**é…ç½®å†…å®¹**:
```json
{
  "project_name": "csv_backtest_ethusdt_1s",
  "description": "ä½¿ç”¨1ç§’Kçº¿CSVæ•°æ®è¿›è¡ŒDDPS-Zç­–ç•¥å›æµ‹ç¤ºä¾‹",

  "data_source": {
    "type": "csv_local",
    "csv_path": "/Users/chenchiyuan/projects/crypto_exchange_news_crawler/data/ETHUSDT-1s-2025-12.csv",
    "interval": "1s",
    "timestamp_unit": "microseconds"
  },

  "backtest": {
    "symbol": "ETHUSDT",
    "interval": "1s",
    "start_date": "2025-12-01",
    "end_date": "2025-12-07"
  },

  "capital_management": {
    "initial_cash": 10000,
    "position_size": 100,
    "commission_rate": 0.001
  },

  "strategies": [
    {
      "id": "strategy_1",
      "type": "ddps_z",
      "enabled_strategies": [1, 2],
      "exits": [
        {
          "type": "ema_regression"
        }
      ]
    }
  ]
}
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®
- [ ] å¯è¢«ProjectLoaderæ­£ç¡®è§£æ

**ä¾èµ–**: TASK-025-011

---

### TASK-025-013: ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

**ç›®æ ‡**: éªŒè¯CSVæ•°æ®æºå›æµ‹å®Œæ•´æµç¨‹

**åŠŸèƒ½ç‚¹è¦†ç›–**: FP-014~022

**æµ‹è¯•æ­¥éª¤**:
```bash
# 1. æ‰§è¡Œå›æµ‹ï¼ˆä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
python manage.py run_strategy_backtest \
  --config strategy_adapter/configs/csv_backtest_example.json

# 2. æ‰§è¡Œå›æµ‹ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
python manage.py run_strategy_backtest \
  --config strategy_adapter/configs/csv_backtest_example.json \
  --save-to-db

# 3. éªŒè¯Webç•Œé¢
# è®¿é—® /backtest/results/ æŸ¥çœ‹ç»“æœ
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å›æµ‹å‘½ä»¤æ­£å¸¸æ‰§è¡Œ
- [ ] æ— Pythonå¼‚å¸¸
- [ ] è¾“å‡ºå›æµ‹ç»Ÿè®¡ä¿¡æ¯
- [ ] --save-to-dbåˆ›å»ºæ•°æ®åº“è®°å½•
- [ ] Webç•Œé¢æ˜¾ç¤ºå›æµ‹ç»“æœ
- [ ] æƒç›Šæ›²çº¿å›¾è¡¨æ­£å¸¸æ¸²æŸ“
- [ ] è®¢å•åˆ—è¡¨æ˜¾ç¤ºæ­£ç¡®

**ä¾èµ–**: TASK-025-012

---

### TASK-025-014: æ€§èƒ½éªŒè¯æµ‹è¯•

**ç›®æ ‡**: éªŒè¯æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡

**åŠŸèƒ½ç‚¹è¦†ç›–**: æ€§èƒ½è¦æ±‚

**æµ‹è¯•å†…å®¹**:

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹è¯•æ–¹æ³• |
|------|------|----------|
| CSVåŠ è½½æ—¶é—´ | <30ç§’ | è®¡æ—¶268ä¸‡è¡ŒåŠ è½½ |
| å†…å­˜å ç”¨ | <2GB | memory_profilerç›‘æ§ |
| æ—¶é—´è¿‡æ»¤ | <100ms | è®¡æ—¶100ä¸‡æ¡è¿‡æ»¤ |
| fetchç¼“å­˜å‘½ä¸­ | <10ms | è®¡æ—¶äºŒæ¬¡è°ƒç”¨ |

**æµ‹è¯•è„šæœ¬**:
```python
import time
import tracemalloc
from ddps_z.datasources import CSVFetcher

def test_performance():
    csv_path = '/path/to/ETHUSDT-1s-2025-12.csv'

    # æµ‹è¯•åŠ è½½æ—¶é—´
    tracemalloc.start()
    start = time.time()

    fetcher = CSVFetcher(csv_path=csv_path, interval='1s')
    klines = fetcher.fetch('ETHUSDT', '1s')

    load_time = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    print(f"åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
    print(f"å†…å­˜å³°å€¼: {peak / 1024 / 1024:.2f}MB")
    print(f"Kçº¿æ•°é‡: {len(klines)}")

    assert load_time < 30, f"åŠ è½½æ—¶é—´è¶…æ ‡: {load_time}ç§’"
    assert peak < 2 * 1024 * 1024 * 1024, f"å†…å­˜è¶…æ ‡: {peak}å­—èŠ‚"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡
- [ ] æ— å†…å­˜æ³„æ¼

**ä¾èµ–**: TASK-025-013

---

## ä»»åŠ¡ä¾èµ–å›¾

```mermaid
graph TD
    subgraph é˜¶æ®µ1["é˜¶æ®µ1: æ ¸å¿ƒè§£æå±‚"]
        T001[TASK-025-001<br>CSVParseråŸºç¡€ç±»]
        T002[TASK-025-002<br>parseæ–¹æ³•å®ç°]
        T003[TASK-025-003<br>CSVParseræµ‹è¯•]
        T004[TASK-025-004<br>æ€§èƒ½ä¼˜åŒ–]
    end

    subgraph é˜¶æ®µ2["é˜¶æ®µ2: æ•°æ®è·å–å±‚"]
        T005[TASK-025-005<br>CSVFetcheråŸºç¡€ç±»]
        T006[TASK-025-006<br>fetchæ–¹æ³•å®ç°]
        T007[TASK-025-007<br>CSVFetcheræµ‹è¯•]
        T008[TASK-025-008<br>æ—¶é—´è¿‡æ»¤ä¼˜åŒ–]
    end

    subgraph é˜¶æ®µ3["é˜¶æ®µ3: å·¥å‚é›†æˆå±‚"]
        T009[TASK-025-009<br>FetcherFactoryæ‰©å±•]
        T010[TASK-025-010<br>æ¨¡å—å¯¼å‡º]
        T011[TASK-025-011<br>ProjectLoaderæ‰©å±•]
    end

    subgraph é˜¶æ®µ4["é˜¶æ®µ4: ç«¯åˆ°ç«¯éªŒè¯"]
        T012[TASK-025-012<br>ç¤ºä¾‹é…ç½®]
        T013[TASK-025-013<br>é›†æˆæµ‹è¯•]
        T014[TASK-025-014<br>æ€§èƒ½éªŒè¯]
    end

    T001 --> T002
    T002 --> T003
    T002 --> T004

    T001 --> T005
    T005 --> T006
    T006 --> T007
    T006 --> T008

    T005 --> T009
    T005 --> T010
    T009 --> T011

    T011 --> T012
    T012 --> T013
    T013 --> T014
```

---

## é£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| CSVåŠ è½½è¶…æ—¶ | é«˜ | ä½ | ä½¿ç”¨pandasä¼˜åŒ–è¯»å–ï¼Œåˆ†æ‰¹å¤„ç† |
| å†…å­˜ä¸è¶³ | é«˜ | ä½ | ç›‘æ§å†…å­˜ï¼Œé¢„è­¦æœºåˆ¶ |
| æ—¶é—´æˆ³æ ¼å¼é”™è¯¯ | ä¸­ | ä½ | é…ç½®æŒ‡å®štimestamp_unit |
| å·¥å‚å‘åå…¼å®¹é—®é¢˜ | ä¸­ | ä½ | å……åˆ†æµ‹è¯•ç°æœ‰Fetcher |

---

## ç›¸å…³æ–‡æ¡£

- PRD: `docs/iterations/025-csv-datasource/prd.md`
- åŠŸèƒ½ç‚¹æ¸…å•: `docs/iterations/025-csv-datasource/function-points.md`
- æ¶æ„è®¾è®¡: `docs/iterations/025-csv-datasource/architecture.md`
- éœ€æ±‚æ¾„æ¸…: `docs/iterations/025-csv-datasource/clarifications.md`
